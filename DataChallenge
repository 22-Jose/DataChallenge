{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11355674,"sourceType":"datasetVersion","datasetId":7106542},{"sourceId":11440834,"sourceType":"datasetVersion","datasetId":7166865},{"sourceId":11454624,"sourceType":"datasetVersion","datasetId":7177140},{"sourceId":11454642,"sourceType":"datasetVersion","datasetId":7177156}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T00:08:57.221155Z","iopub.execute_input":"2025-04-17T00:08:57.221506Z","iopub.status.idle":"2025-04-17T00:08:59.875088Z","shell.execute_reply.started":"2025-04-17T00:08:57.221479Z","shell.execute_reply":"2025-04-17T00:08:59.874099Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vfinal/Final.parquet\n/kaggle/input/serie-diaria/municipios_diaria_SinConc.parquet\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df = pd.read_parquet(\"/kaggle/input/vfinal/Final.parquet\")\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T00:09:03.983262Z","iopub.execute_input":"2025-04-17T00:09:03.983732Z","iopub.status.idle":"2025-04-17T00:09:09.035624Z","shell.execute_reply.started":"2025-04-17T00:09:03.983705Z","shell.execute_reply":"2025-04-17T00:09:09.034709Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"         Nombre_Municipio_IPS Nombre_Departamento_IPS  \\\n0                    MEDELLIN               ANTIOQUIA   \n1                    MEDELLIN               ANTIOQUIA   \n2                      BOGOTA             BOGOTA D.C.   \n3                     POPAYAN                   CAUCA   \n4            SANTIAGO DE CALI         VALLE DEL CAUCA   \n...                       ...                     ...   \n11716411             MEDELLIN               ANTIOQUIA   \n11716412             MEDELLIN               ANTIOQUIA   \n11716413             MEDELLIN               ANTIOQUIA   \n11716414               BOGOTA                   HUILA   \n11716415          BUCARAMANGA               SANTANDER   \n\n         Nombre_Municipio_Establecimiento Nombre_Departamento_Establecimiento  \\\n0                                       ?                                   ?   \n1                                       ?                                   ?   \n2                                       ?                                   ?   \n3                                       ?                                   ?   \n4                                       ?                                   ?   \n...                                   ...                                 ...   \n11716411                         MEDELLIN                           ANTIOQUIA   \n11716412                                ?                                   ?   \n11716413                                ?                                   ?   \n11716414                                ?                                   ?   \n11716415                                ?                                   ?   \n\n              FECHA_ATENCION                  Concepto_Factura_Desc  Cantidad  \\\n0        2019-07-12 06:20:00       MSI -MEDICO SEGUIMIENTO INTEGRAL       1.0   \n1        2019-06-18 11:08:00               SESIONES DE FISIOTERAPIA       2.0   \n2        2019-03-05 07:20:00                   CONSULTA ORTOPEDISTA       1.0   \n3        2019-10-05 10:03:00                 CONSULTA NO PROGRAMADA       1.0   \n4        2019-06-25 07:13:00                            RADIOGRAFIA       1.0   \n...                      ...                                    ...       ...   \n11716411 2024-10-01 12:00:00                   CONSULTA ORTOPEDISTA       1.0   \n11716412 2024-12-17 20:10:30              CALIFICACIÓN DE ORIGEN AT       1.0   \n11716413 2024-10-02 00:00:00               SESIONES DE FISIOTERAPIA       2.0   \n11716414 2024-11-07 00:00:00  CONSULTA PSIQUIATRIA TELEMEDICINA (L)       1.0   \n11716415 2024-10-01 15:23:00               SESIONES DE FISIOTERAPIA      10.0   \n\n          Geogra_Municipio_Id         MUNICIPIO  \n0                         586            BOGOTA  \n1                         586            BOGOTA  \n2                         586            BOGOTA  \n3                         834  SANTIAGO DE CALI  \n4                        2546           GUACARI  \n...                       ...               ...  \n11716411                  586            BOGOTA  \n11716412                  586            BOGOTA  \n11716413                  586            BOGOTA  \n11716414                 4584             NEIVA  \n11716415                 3875           LEBRIJA  \n\n[11716416 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Nombre_Municipio_IPS</th>\n      <th>Nombre_Departamento_IPS</th>\n      <th>Nombre_Municipio_Establecimiento</th>\n      <th>Nombre_Departamento_Establecimiento</th>\n      <th>FECHA_ATENCION</th>\n      <th>Concepto_Factura_Desc</th>\n      <th>Cantidad</th>\n      <th>Geogra_Municipio_Id</th>\n      <th>MUNICIPIO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MEDELLIN</td>\n      <td>ANTIOQUIA</td>\n      <td>?</td>\n      <td>?</td>\n      <td>2019-07-12 06:20:00</td>\n      <td>MSI -MEDICO SEGUIMIENTO INTEGRAL</td>\n      <td>1.0</td>\n      <td>586</td>\n      <td>BOGOTA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MEDELLIN</td>\n      <td>ANTIOQUIA</td>\n      <td>?</td>\n      <td>?</td>\n      <td>2019-06-18 11:08:00</td>\n      <td>SESIONES DE FISIOTERAPIA</td>\n      <td>2.0</td>\n      <td>586</td>\n      <td>BOGOTA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BOGOTA</td>\n      <td>BOGOTA D.C.</td>\n      <td>?</td>\n      <td>?</td>\n      <td>2019-03-05 07:20:00</td>\n      <td>CONSULTA ORTOPEDISTA</td>\n      <td>1.0</td>\n      <td>586</td>\n      <td>BOGOTA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>POPAYAN</td>\n      <td>CAUCA</td>\n      <td>?</td>\n      <td>?</td>\n      <td>2019-10-05 10:03:00</td>\n      <td>CONSULTA NO PROGRAMADA</td>\n      <td>1.0</td>\n      <td>834</td>\n      <td>SANTIAGO DE CALI</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SANTIAGO DE CALI</td>\n      <td>VALLE DEL CAUCA</td>\n      <td>?</td>\n      <td>?</td>\n      <td>2019-06-25 07:13:00</td>\n      <td>RADIOGRAFIA</td>\n      <td>1.0</td>\n      <td>2546</td>\n      <td>GUACARI</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11716411</th>\n      <td>MEDELLIN</td>\n      <td>ANTIOQUIA</td>\n      <td>MEDELLIN</td>\n      <td>ANTIOQUIA</td>\n      <td>2024-10-01 12:00:00</td>\n      <td>CONSULTA ORTOPEDISTA</td>\n      <td>1.0</td>\n      <td>586</td>\n      <td>BOGOTA</td>\n    </tr>\n    <tr>\n      <th>11716412</th>\n      <td>MEDELLIN</td>\n      <td>ANTIOQUIA</td>\n      <td>?</td>\n      <td>?</td>\n      <td>2024-12-17 20:10:30</td>\n      <td>CALIFICACIÓN DE ORIGEN AT</td>\n      <td>1.0</td>\n      <td>586</td>\n      <td>BOGOTA</td>\n    </tr>\n    <tr>\n      <th>11716413</th>\n      <td>MEDELLIN</td>\n      <td>ANTIOQUIA</td>\n      <td>?</td>\n      <td>?</td>\n      <td>2024-10-02 00:00:00</td>\n      <td>SESIONES DE FISIOTERAPIA</td>\n      <td>2.0</td>\n      <td>586</td>\n      <td>BOGOTA</td>\n    </tr>\n    <tr>\n      <th>11716414</th>\n      <td>BOGOTA</td>\n      <td>HUILA</td>\n      <td>?</td>\n      <td>?</td>\n      <td>2024-11-07 00:00:00</td>\n      <td>CONSULTA PSIQUIATRIA TELEMEDICINA (L)</td>\n      <td>1.0</td>\n      <td>4584</td>\n      <td>NEIVA</td>\n    </tr>\n    <tr>\n      <th>11716415</th>\n      <td>BUCARAMANGA</td>\n      <td>SANTANDER</td>\n      <td>?</td>\n      <td>?</td>\n      <td>2024-10-01 15:23:00</td>\n      <td>SESIONES DE FISIOTERAPIA</td>\n      <td>10.0</td>\n      <td>3875</td>\n      <td>LEBRIJA</td>\n    </tr>\n  </tbody>\n</table>\n<p>11716416 rows × 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:51:27.797042Z","iopub.execute_input":"2025-04-15T15:51:27.797421Z","iopub.status.idle":"2025-04-15T15:51:27.803900Z","shell.execute_reply.started":"2025-04-15T15:51:27.797393Z","shell.execute_reply":"2025-04-15T15:51:27.802681Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Comenzamos con el Proceso de limpieza de los datos.","metadata":{}},{"cell_type":"code","source":"#Para ver variables categoricas y numericas\ndf.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:51:27.805870Z","iopub.execute_input":"2025-04-15T15:51:27.806271Z","iopub.status.idle":"2025-04-15T15:51:27.843485Z","shell.execute_reply.started":"2025-04-15T15:51:27.806241Z","shell.execute_reply":"2025-04-15T15:51:27.842186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Verificamos los valores Nulos","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:51:27.844555Z","iopub.execute_input":"2025-04-15T15:51:27.844819Z","iopub.status.idle":"2025-04-15T15:51:31.496738Z","shell.execute_reply.started":"2025-04-15T15:51:27.844796Z","shell.execute_reply":"2025-04-15T15:51:31.495577Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Esta verificacion de datos nulos puede ser un poco engañosa, pues en el dataset hay datos faltantes disfrazados a los que tambien se les eaplicara un tratamiento especial.","metadata":{}},{"cell_type":"markdown","source":"2. Revisar valores únicos y consistencia en texto\nEsto es útil para detectar errores de escritura o inconsistencias en nombres de municipios o departamentos:","metadata":{}},{"cell_type":"code","source":"print(df['Nombre_Municipio_IPS'].value_counts(dropna=False))\nprint(df['Nombre_Departamento_IPS'].value_counts(dropna=False))\nprint(df['Concepto_Factura_Desc'].value_counts(dropna=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:51:31.497734Z","iopub.execute_input":"2025-04-15T15:51:31.498058Z","iopub.status.idle":"2025-04-15T15:51:32.014454Z","shell.execute_reply.started":"2025-04-15T15:51:31.498032Z","shell.execute_reply":"2025-04-15T15:51:32.013325Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. Estandarizar nombres de texto (eliminamos los espacios innecesario y ademas pasamos todo a mayusculas)","metadata":{}},{"cell_type":"code","source":"# Limpiar y estandarizar texto: mayúsculas, sin espacios al inicio/fin, y con guiones bajos en vez de espacios internos\ncols_to_clean = [\n    'Nombre_Municipio_IPS',\n    'Nombre_Departamento_IPS',\n    'Concepto_Factura_Desc',\n    'MUNICIPIO'\n]\n\n# Estandarización de texto: mayúsculas, sin espacios, guiones bajos\nfor col in cols_to_clean:\n    df[col] = df[col].astype(str).str.upper().str.strip().str.replace(' ', '_')\n\n# Reemplazo de valores no válidos o mal cargados por NaN\ndf[cols_to_clean] = df[cols_to_clean].replace(\n    r'^\\s*$|^NONE$|^\\?$|^SIN_DATOS$|^NOMBRE_.*$', \n    np.nan,\n    regex=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:52:07.904895Z","iopub.execute_input":"2025-04-15T15:52:07.905372Z","iopub.status.idle":"2025-04-15T15:53:23.373209Z","shell.execute_reply.started":"2025-04-15T15:52:07.905341Z","shell.execute_reply":"2025-04-15T15:53:23.371990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PARA VERIFICAR LOS CAMBIOS\ndf.head(50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:53:48.554814Z","iopub.execute_input":"2025-04-15T15:53:48.555279Z","iopub.status.idle":"2025-04-15T15:53:48.580781Z","shell.execute_reply.started":"2025-04-15T15:53:48.555246Z","shell.execute_reply":"2025-04-15T15:53:48.579486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:53:53.065539Z","iopub.execute_input":"2025-04-15T15:53:53.065979Z","iopub.status.idle":"2025-04-15T15:53:57.696119Z","shell.execute_reply.started":"2025-04-15T15:53:53.065944Z","shell.execute_reply":"2025-04-15T15:53:57.695193Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Las variables FECHA_ATENCION y Concepto_Factura_Desc contienen muy pocos valores nulos, estos valores los podemos eliminar, pues eliminar estos registros no afectará significativamente el análisis.","metadata":{}},{"cell_type":"code","source":"# Filtrar y copiar para evitar\ndf = df.dropna(subset=['FECHA_ATENCION', 'Concepto_Factura_Desc']).copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:54:22.408581Z","iopub.execute_input":"2025-04-15T15:54:22.408986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:51:32.028408Z","iopub.status.idle":"2025-04-15T15:51:32.029307Z","shell.execute_reply":"2025-04-15T15:51:32.029041Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Analisis Exploratorio","metadata":{}},{"cell_type":"code","source":"from ydata_profiling import ProfileReport\n\n# Crear el reporte\nprofile = ProfileReport(df, title='Reporte de Exploración', explorative=True)\n\n# Mostrar en notebook\nprofile.to_notebook_iframe()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:51:32.031772Z","iopub.status.idle":"2025-04-15T15:51:32.032342Z","shell.execute_reply":"2025-04-15T15:51:32.032147Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Veamos como se comporta un Concepto dado en un municipio dado a lo largo de nuestra serie de tiempo.","metadata":{}},{"cell_type":"code","source":"# Parámetros para filtrar\nmunicipio = 'MEDELLIN'\nconcepto = 'SESIONES_DE_FISIOTERAPIA'\n\n# Filtro\ndf_viz = serie[\n    (serie['MUNICIPIO'] == municipio) & \n    (serie['Concepto_Factura_Desc'] == concepto)\n]\n\n# Convertir a serie temporal con índice de fecha\nserie_uni = df_viz.set_index('MES').asfreq('MS').fillna(0)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:51:32.033310Z","iopub.status.idle":"2025-04-15T15:51:32.033657Z","shell.execute_reply":"2025-04-15T15:51:32.033538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gráfico\nplt.figure(figsize=(12,6))\nplt.plot(df_viz['MES'], df_viz['Cantidad'], marker='o')\nplt.title(f'{concepto.replace(\"_\", \" \").title()} en {municipio.title()} (Mensual)')\nplt.xlabel('Mes')\nplt.ylabel('Cantidad')\nplt.grid(True)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Series Mensuales, Semanales, Diarias.","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# Instalar solo lo que falta (neuralforecast)\n#!pip install neuralforecast\n#!pip install statsmodels # por si no esta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T23:57:54.672356Z","iopub.execute_input":"2025-04-16T23:57:54.672669Z","iopub.status.idle":"2025-04-16T23:57:59.691765Z","shell.execute_reply.started":"2025-04-16T23:57:54.672646Z","shell.execute_reply":"2025-04-16T23:57:59.690809Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.7/197.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Mejoras en configuración inicial\nimport os\nimport warnings\nimport torch\nimport matplotlib.pyplot as plt\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NBEATS, NBEATSx  # Añadido NBEATSx (versión mejorada)\nfrom neuralforecast.losses.pytorch import MAE, MASE, SMAPE  # Añadidas más funciones de pérdida\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler  # Añadido RobustScaler para mayor robustez\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.tsa.seasonal import STL\nimport gc\nimport joblib\nfrom tqdm import tqdm  # Para barras de progreso","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T23:58:04.136498Z","iopub.execute_input":"2025-04-16T23:58:04.136818Z","iopub.status.idle":"2025-04-16T23:58:04.451351Z","shell.execute_reply.started":"2025-04-16T23:58:04.136796Z","shell.execute_reply":"2025-04-16T23:58:04.450600Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Suprimir advertencias no críticas\nwarnings.filterwarnings('ignore')\n\n# Configuración avanzada de rendimiento\ntorch.set_float32_matmul_precision('high')\ntorch.backends.cudnn.benchmark = True  # Acelera operaciones convolutivas\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Usando dispositivo: {device}')\n\n# Configuración de memoria para GPU si está disponible\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(f'GPU disponible: {torch.cuda.get_device_name(1)}')\n    print(f'Memoria GPU: {torch.cuda.get_device_properties(1).total_memory / 1e9:.2f} GB')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Serie Semanal","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='pandas') # Ignorar advertencias sobre Period\n\ndef load_and_preprocess_detailed_weekly(file_path, covid_start_date='2020-03-01', covid_end_date='2022-06-30',\n                                       filter_min_weeks_active=5, filter_min_total_qty=10):\n    \"\"\"\n    Carga y preprocesa datos de demanda desde archivo Parquet, agregando\n    SEMANALMENTE por Municipio y Concepto de Factura.\n    Crea un ID único por serie, incluye flag COVID y aplica filtros opcionales.\n\n    Args:\n        file_path: Ruta al archivo Parquet.\n        covid_start_date: Fecha de inicio (inclusive) para marcar como periodo COVID.\n        covid_end_date: Fecha de fin (inclusive) para marcar como periodo COVID.\n        filter_min_weeks_active: int o None. Si es int, filtra series con menos semanas de actividad (>0).\n        filter_min_total_qty: int o None. Si es int, filtra series con cantidad total menor a este valor.\n    \"\"\"\n    # Leer columnas necesarias desde Parquet\n    print(\"Cargando datos...\")\n    required_columns = ['FECHA_ATENCION', 'MUNICIPIO', 'Concepto_Factura_Desc', 'Cantidad']\n    try:\n        df = pd.read_parquet(file_path, columns=required_columns)\n    except Exception as e:\n        raise FileNotFoundError(f\"Error al leer el archivo Parquet '{file_path}': {e}\")\n\n    # --- Verificación inicial ---\n    if df.empty:\n        raise ValueError(\"El archivo Parquet está vacío o no contiene las columnas requeridas.\")\n    print(f\"Columnas cargadas: {df.columns.tolist()}\")\n    print(f\"Filas iniciales: {len(df):,}\")\n\n    # Limpiar nombres y manejar nulos iniciales\n    df.columns = df.columns.str.strip()\n    for col in required_columns:\n        if col not in df.columns:\n            raise ValueError(f\"La columna requerida '{col}' no se encontró en el archivo.\")\n    df.dropna(subset=['FECHA_ATENCION', 'MUNICIPIO', 'Concepto_Factura_Desc'], inplace=True)\n    df['Cantidad'] = df['Cantidad'].fillna(0)\n\n    print(f\"Filas después de eliminar nulos en claves: {len(df):,}\")\n    if df.empty:\n        print(\"Advertencia: No quedan datos después de eliminar nulos en columnas clave.\")\n        return pd.DataFrame(), [], [], []\n\n    # Optimizar tipos de datos\n    print(\"Optimizando tipos de datos...\")\n    df['MUNICIPIO'] = df['MUNICIPIO'].astype('category')\n    df['Concepto_Factura_Desc'] = df['Concepto_Factura_Desc'].astype('category')\n    df['Cantidad'] = pd.to_numeric(df['Cantidad'], errors='coerce').fillna(0).astype('float32')\n    df['FECHA_ATENCION'] = pd.to_datetime(df['FECHA_ATENCION'], errors='coerce')\n    df.dropna(subset=['FECHA_ATENCION'], inplace=True) # Eliminar fechas inválidas\n\n    print(f\"Filas después de eliminar fechas inválidas: {len(df):,}\")\n    if df.empty:\n        print(\"Advertencia: No quedan datos después de eliminar fechas inválidas.\")\n        return pd.DataFrame(), [], [], []\n\n    print(f\"Dataset optimizado: {df.shape[0]:,} filas, {df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n\n    # --- Agregación SEMANAL ---\n    print(\"Agregando datos por SEMANA (inicio Lunes), municipio y concepto...\")\n    # Usamos dt.to_period('W') para obtener la semana ISO y .dt.to_timestamp() para obtener\n    # el timestamp del LUNES de esa semana, lo que facilita el rellenado posterior.\n    df['ds'] = df['FECHA_ATENCION'].dt.to_period('W').dt.to_timestamp() # <-- CAMBIO CLAVE: Semana\n    grouping_cols = ['MUNICIPIO', 'Concepto_Factura_Desc', 'ds']\n\n    df_semanal = (\n        df.groupby(grouping_cols, observed=False)['Cantidad'] # observed=False puede ser más seguro con categories\n        .sum()\n        .reset_index()\n        .rename(columns={'Cantidad': 'y'})\n    )\n    print(f\"Filas después de agregar SEMANALMENTE: {len(df_semanal):,}\")\n\n    if df_semanal.empty:\n       print(\"Advertencia: No hay datos después de la agregación semanal.\")\n       return pd.DataFrame(), [], [], []\n\n    # Crear el Identificador Único de Serie (ID_Serie)\n    print(\"Creando ID_Serie...\")\n    df_semanal['ID_Serie'] = df_semanal['MUNICIPIO'].astype(str) + '_' + df_semanal['Concepto_Factura_Desc'].astype(str)\n    df_semanal['ID_Serie'] = df_semanal['ID_Serie'].astype('category')\n\n    # --- Filtrado Opcional de Series (ANTES de completar) ---\n    num_series_before_filter = df_semanal['ID_Serie'].nunique()\n    print(f\"Número de series únicas ANTES del filtrado: {num_series_before_filter:,}\")\n\n    if filter_min_weeks_active is not None and filter_min_weeks_active > 0:\n        print(f\"Filtrando series con menos de {filter_min_weeks_active} semanas activas (y>0)...\")\n        weeks_active = df_semanal[df_semanal['y'] > 0].groupby('ID_Serie')['ds'].nunique()\n        series_to_keep_active = weeks_active[weeks_active >= filter_min_weeks_active].index\n        df_semanal = df_semanal[df_semanal['ID_Serie'].isin(series_to_keep_active)]\n        print(f\"  Series restantes después del filtro de actividad: {df_semanal['ID_Serie'].nunique():,}\")\n\n    if filter_min_total_qty is not None and filter_min_total_qty > 0:\n         print(f\"Filtrando series con cantidad total menor a {filter_min_total_qty}...\")\n         total_qty = df_semanal.groupby('ID_Serie')['y'].sum()\n         series_to_keep_qty = total_qty[total_qty >= filter_min_total_qty].index\n         df_semanal = df_semanal[df_semanal['ID_Serie'].isin(series_to_keep_qty)]\n         print(f\"  Series restantes después del filtro de cantidad: {df_semanal['ID_Serie'].nunique():,}\")\n\n    num_series_after_filter = df_semanal['ID_Serie'].nunique()\n    if num_series_after_filter == 0:\n        print(\"Advertencia: El filtrado eliminó todas las series.\")\n        return pd.DataFrame(), [], [], []\n    print(f\"Número de series únicas DESPUÉS del filtrado: {num_series_after_filter:,}\")\n\n\n    # --- Completar series temporales SEMANALES para cada ID_Serie filtrada ---\n    print(\"Completando series temporales SEMANALES por ID_Serie filtrada...\")\n    min_week = df_semanal['ds'].min()\n    max_week = df_semanal['ds'].max()\n    # Usar freq='W-MON' para asegurar que coincide con el timestamp generado por .to_period('W').dt.to_timestamp()\n    week_range = pd.date_range(min_week, max_week, freq='W-MON') # <-- CAMBIO CLAVE: Frecuencia Semanal (Lunes)\n\n    unique_series_ids_filtered = df_semanal['ID_Serie'].unique()\n\n    # Mapa para restaurar detalles\n    map_id_to_details = df_semanal[['ID_Serie', 'MUNICIPIO', 'Concepto_Factura_Desc']].drop_duplicates().set_index('ID_Serie')\n\n    # MultiIndex con combinaciones semana-serie filtrada\n    idx = pd.MultiIndex.from_product(\n        [unique_series_ids_filtered, week_range],\n        names=['ID_Serie', 'ds']\n    )\n\n    # Reindexar y rellenar\n    serie_completa = (\n        df_semanal.set_index(['ID_Serie', 'ds'])['y']\n        .reindex(idx, fill_value=0)\n        .reset_index()\n    )\n\n    # Añadir detalles de Municipio/Concepto\n    serie_completa = pd.merge(serie_completa, map_id_to_details.reset_index(), on='ID_Serie', how='left')\n\n    print(f\"Filas después de completar series SEMANALES: {len(serie_completa):,}\")\n    print(f\"Uso de memoria estimado: {serie_completa.memory_usage(deep=True).sum() / 1e6:.2f} MB\") # Mucho menor ahora\n\n    # --- Añadir características temporales SEMANALES ---\n    print(\"Añadiendo características temporales semanales...\")\n    serie_completa['año'] = serie_completa['ds'].dt.year\n    serie_completa['mes'] = serie_completa['ds'].dt.month # El mes puede seguir siendo útil\n    serie_completa['semana_año'] = serie_completa['ds'].dt.isocalendar().week.astype(int) # <-- Característica semanal clave\n    serie_completa['trimestre'] = serie_completa['ds'].dt.quarter\n    # Se eliminan características diarias\n\n    # Añadir índice de tiempo numérico (número de semanas transcurridas por serie)\n    print(\"Añadiendo índice de tiempo numérico (time_idx)...\")\n    serie_completa = serie_completa.sort_values(by=['ID_Serie', 'ds'])\n    serie_completa['time_idx'] = serie_completa.groupby('ID_Serie').cumcount()\n\n    # --- Añadir flag para el periodo COVID ---\n    print(\"Añadiendo flag COVID...\")\n    start_dt = pd.to_datetime(covid_start_date)\n    end_dt = pd.to_datetime(covid_end_date)\n    # La comparación funciona igual con las fechas de inicio de semana\n    serie_completa['es_covid'] = (\n        (serie_completa['ds'] >= start_dt) & (serie_completa['ds'] <= end_dt)\n    ).astype(int)\n\n    # Liberar memoria\n    del df, df_semanal\n    gc.collect()\n\n    # Obtener listas únicas finales (basadas en el df filtrado y completado)\n    municipios_final = serie_completa['MUNICIPIO'].unique().tolist()\n    conceptos_final = serie_completa['Concepto_Factura_Desc'].unique().tolist()\n    series_ids_final = serie_completa['ID_Serie'].unique().tolist() # Ya son las filtradas\n\n    print(f\"Procesamiento SEMANAL completo para {len(series_ids_final)} series únicas (Municipio-Concepto)\")\n    print(f\"Total de registros en la serie final: {serie_completa.shape[0]:,}\") # <-- ESTE NÚMERO DEBERÍA SER MUCHO MENOR\n    print(f\"Periodo cubierto (inicio de semana): {min_week.strftime('%Y-%m-%d')} a {max_week.strftime('%Y-%m-%d')}\")\n    print(f\"Columnas finales: {serie_completa.columns.tolist()}\")\n\n    return serie_completa, municipios_final, conceptos_final, series_ids_final\n\ndef visualize_specific_series_weekly(serie_completa, id_serie_to_visualize):\n    \"\"\"\n    Visualiza la demanda SEMANAL de una serie específica (Municipio-Concepto).\n    \"\"\"\n    plt.figure(figsize=(16, 7)) # Ancho adecuado para series semanales\n\n    datos_serie = serie_completa[serie_completa['ID_Serie'] == id_serie_to_visualize].sort_values('ds')\n\n    if datos_serie.empty:\n        print(f\"No se encontraron datos para ID_Serie: {id_serie_to_visualize}\")\n        return\n\n    plt.plot(datos_serie['ds'], datos_serie['y'], marker='.', linestyle='-', label='Demanda Semanal (y)', markersize=5)\n\n    # Resaltar periodo COVID\n    if 'es_covid' in datos_serie.columns:\n        covid_period = datos_serie[datos_serie['es_covid'] == 1]\n        if not covid_period.empty:\n            plt.fill_between(datos_serie['ds'], 0, datos_serie['y'].max() * 1.05,\n                             where=datos_serie['es_covid'] == 1,\n                             color='red', alpha=0.15, label='Periodo COVID estimado')\n\n    plt.title(f'Demanda SEMANAL - Serie: {id_serie_to_visualize}')\n    plt.xlabel('Semana (Inicio Lunes)')\n    plt.ylabel('Cantidad (y)')\n    plt.grid(True, linestyle='--', alpha=0.6)\n    plt.xticks(rotation=30, ha='right')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n# --- Uso ---\nfile_path = '/kaggle/input/vfinal/Final.parquet' # Path\n\n# Puedes empezar sin filtros (None) o con valores bajos y ver cuántas series quedan\nmin_semanas_activas = 10 # Ejemplo: requerir al menos 10 semanas con alguna atención\nmin_cantidad_total = 20  # Ejemplo: requerir al menos 20 atenciones totales en toda la historia\n\ntry:\n    # Llamar a la función semanal con los filtros\n    serie_final_semanal, municipios, conceptos, series_ids = load_and_preprocess_detailed_weekly(\n        file_path,\n        filter_min_weeks_active=min_semanas_activas,\n        filter_min_total_qty=min_cantidad_total\n    )\n\n    if not serie_final_semanal.empty:\n        print(\"\\n--- Resumen del DataFrame Procesado SEMANAL ---\")\n        print(serie_final_semanal.head())\n        print(\"\\nInformación del DataFrame:\")\n        serie_final_semanal.info(verbose=False, memory_usage='deep')\n        print(f\"\\nNúmero de Municipios únicos: {len(municipios)}\")\n        print(f\"Número de Conceptos únicos: {len(conceptos)}\")\n        print(f\"\\nNúmero de Series únicas (ID_Serie) DESPUÉS de filtrar: {len(series_ids)}\") # <-- Número clave\n        print(f\"Número TOTAL de filas final: {len(serie_final_semanal):,}\") # <-- Número clave\n        print(f\"Rango de fechas (inicio semana): {serie_final_semanal['ds'].min()} a {serie_final_semanal['ds'].max()}\")\n\n\n        if series_ids:\n            id_ejemplo = series_ids[np.random.randint(0, len(series_ids))] # Elegir una serie al azar de las filtradas\n            print(f\"\\nVisualizando ejemplo SEMANAL para la serie: {id_ejemplo}\")\n            visualize_specific_series_weekly(serie_final_semanal, id_ejemplo)\n\nexcept FileNotFoundError as fnf:\n    print(f\"Error: {fnf}\")\nexcept ValueError as ve:\n    print(f\"Error de datos: {ve}\")\nexcept Exception as e:\n    print(f\"Ocurrió un error inesperado durante el procesamiento semanal: {e}\")\n    import traceback\n    traceback.print_exc()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizar semanal","metadata":{}},{"cell_type":"code","source":"if series_ids:\n            id_ejemplo = series_ids[0] # Elegir una serie\n            print(f\"\\nVisualizando ejemplo SEMANAL para la serie: {id_ejemplo}\")\n            visualize_specific_series_weekly(serie_final_semanal, id_ejemplo)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modelo N-Beats","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport time\n\n# Verificar si hay GPU disponible\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Usando dispositivo: {device}\")\n\nclass Block(nn.Module):\n    \"\"\"\n    Bloque N-BEATS básico\n    \"\"\"\n    def __init__(self, input_size, theta_size, hidden_layer_size, nb_layers):\n        super().__init__()\n        \n        self.nb_layers = nb_layers\n        self.hidden_layer_size = hidden_layer_size\n        \n        # Definición de la red de capas ocultas\n        self.layers = nn.ModuleList([nn.Linear(input_size, hidden_layer_size)])\n        for _ in range(nb_layers - 1):\n            self.layers.append(nn.Linear(hidden_layer_size, hidden_layer_size))\n        \n        # Capas de salida para backcast y forecast\n        self.theta_backcast = nn.Linear(hidden_layer_size, input_size)  # Mismo tamaño que input\n        self.theta_forecast = nn.Linear(hidden_layer_size, theta_size)  # Tamaño variable para forecast\n\n    def forward(self, x):\n        # Propagación a través de capas ocultas\n        block_input = x\n        for layer in self.layers:\n            block_input = torch.relu(layer(block_input))\n        \n        # Separación en backcast y forecast\n        backcast = self.theta_backcast(block_input)\n        forecast = self.theta_forecast(block_input)\n        \n        return backcast, forecast\n\nclass Stack(nn.Module):\n    \"\"\"\n    Pila de bloques N-BEATS\n    \"\"\"\n    def __init__(self, input_size, forecast_length, hidden_layer_size, nb_blocks, nb_layers):\n        super().__init__()\n        \n        self.blocks = nn.ModuleList([\n            Block(\n                input_size=input_size,\n                theta_size=forecast_length,  # Tamaño del forecast directamente\n                hidden_layer_size=hidden_layer_size,\n                nb_layers=nb_layers\n            ) for _ in range(nb_blocks)\n        ])\n\n    def forward(self, x):\n        residuals = x.clone()\n        forecast = torch.zeros(x.size(0), self.blocks[0].theta_forecast.out_features).to(device)\n        \n        # Flujo a través de los bloques\n        for block in self.blocks:\n            backcast, block_forecast = block(residuals)\n            residuals = residuals - backcast  # Actualizar residuos\n            forecast = forecast + block_forecast  # Acumular forecast\n            \n        return forecast\n\nclass NBeatsNet(nn.Module):\n    \"\"\"\n    Red N-BEATS completa con múltiples pilas\n    \"\"\"\n    def __init__(self, input_size, forecast_length, hidden_layer_size=128, \n                 nb_blocks_per_stack=3, nb_layers=4, nb_stacks=2):\n        super().__init__()\n        \n        self.stacks = nn.ModuleList([\n            Stack(\n                input_size=input_size,\n                forecast_length=forecast_length,\n                hidden_layer_size=hidden_layer_size,\n                nb_blocks=nb_blocks_per_stack,\n                nb_layers=nb_layers\n            ) for _ in range(nb_stacks)\n        ])\n\n    def forward(self, x):\n        # Inicializar forecast con tamaño correcto\n        forecast = torch.zeros(x.size(0), self.stacks[0].blocks[0].theta_forecast.out_features).to(device)\n        \n        # Flujo a través de las pilas\n        for stack in self.stacks:\n            stack_forecast = stack(x)\n            forecast = forecast + stack_forecast\n            \n        return forecast\n\nclass NBeatsTrainer:\n    \"\"\"\n    Entrenador para modelo N-BEATS\n    \"\"\"\n    def __init__(self, input_size, forecast_length, hidden_layer_size=128, \n                 nb_blocks_per_stack=3, nb_layers=4, nb_stacks=2, \n                 learning_rate=1e-3, batch_size=32):\n        self.input_size = input_size\n        self.forecast_length = forecast_length\n        self.batch_size = batch_size\n        \n        # Creamos el modelo\n        self.model = NBeatsNet(\n            input_size=input_size,\n            forecast_length=forecast_length,\n            hidden_layer_size=hidden_layer_size,\n            nb_blocks_per_stack=nb_blocks_per_stack,\n            nb_layers=nb_layers,\n            nb_stacks=nb_stacks\n        ).to(device)\n        \n        # Optimizador y función de pérdida\n        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n        self.criterion = nn.MSELoss()\n        \n        # Para normalización\n        self.scaler = StandardScaler()\n        \n    def create_sequences(self, data, seq_length):\n        \"\"\"Transforma una serie temporal en secuencias de entrada-salida\"\"\"\n        xs, ys = [], []\n        \n        for i in range(len(data) - seq_length - self.forecast_length + 1):\n            x = data[i:i+seq_length]\n            y = data[i+seq_length:i+seq_length+self.forecast_length]\n            xs.append(x)\n            ys.append(y)\n            \n        return np.array(xs), np.array(ys)\n    \n    def prepare_data(self, series):\n        \"\"\"Prepara los datos para el entrenamiento\"\"\"\n        # Normalizamos los datos\n        series_scaled = self.scaler.fit_transform(series.reshape(-1, 1)).flatten()\n        \n        # Creamos secuencias\n        x, y = self.create_sequences(series_scaled, self.input_size)\n        \n        # Convertimos a tensores\n        x_tensor = torch.FloatTensor(x).to(device)\n        y_tensor = torch.FloatTensor(y).to(device)\n        \n        # Creamos el dataset y dataloader\n        dataset = TensorDataset(x_tensor, y_tensor)\n        dataloader = DataLoader(dataset, batch_size=min(self.batch_size, len(dataset)), shuffle=True)\n        \n        return dataloader\n    \n    def train(self, train_series, epochs=100, patience=10):\n        \"\"\"Entrena el modelo\"\"\"\n        dataloader = self.prepare_data(train_series)\n        \n        best_loss = float('inf')\n        patience_counter = 0\n        train_losses = []\n        \n        print(f\"Secuencias de entrenamiento: {len(dataloader.dataset)}\")\n        \n        for epoch in range(epochs):\n            self.model.train()\n            epoch_losses = []\n            \n            # Barra de progreso para el entrenamiento\n            with tqdm(dataloader, unit=\"batch\") as tepoch:\n                for x_batch, y_batch in tepoch:\n                    tepoch.set_description(f\"Epoch {epoch+1}/{epochs}\")\n                    \n                    # Forward\n                    self.optimizer.zero_grad()\n                    outputs = self.model(x_batch)\n                    \n                    # Asegurar que outputs y y_batch tienen la misma forma\n                    if outputs.shape != y_batch.shape:\n                        print(f\"Error de forma: outputs {outputs.shape}, y_batch {y_batch.shape}\")\n                        continue\n                    \n                    loss = self.criterion(outputs, y_batch)\n                    \n                    # Backward\n                    loss.backward()\n                    self.optimizer.step()\n                    \n                    # Guardar pérdida\n                    epoch_losses.append(loss.item())\n                    tepoch.set_postfix(loss=loss.item())\n            \n            # Pérdida promedio de la época\n            if epoch_losses:\n                avg_loss = np.mean(epoch_losses)\n                train_losses.append(avg_loss)\n                print(f\"Época {epoch+1}/{epochs}, Pérdida: {avg_loss:.6f}\")\n                \n                # Early stopping\n                if avg_loss < best_loss:\n                    best_loss = avg_loss\n                    patience_counter = 0\n                    # Guardar el mejor modelo\n                    torch.save(self.model.state_dict(), 'best_nbeats_model.pth')\n                else:\n                    patience_counter += 1\n                    \n                if patience_counter >= patience:\n                    print(f\"Early stopping en época {epoch+1}\")\n                    break\n        \n        # Cargar el mejor modelo\n        if os.path.exists('best_nbeats_model.pth'):\n            self.model.load_state_dict(torch.load('best_nbeats_model.pth'))\n        \n        return train_losses\n    \n    def predict(self, history, forecast_steps=None):\n        \"\"\"\n        Genera predicciones a partir de datos históricos\n        \"\"\"\n        if forecast_steps is None:\n            forecast_steps = self.forecast_length\n            \n        self.model.eval()\n        \n        # Normalizar datos históricos\n        history_scaled = self.scaler.transform(history.reshape(-1, 1)).flatten()\n        \n        # Si la historia es más corta que input_size, rellenamos con ceros\n        if len(history_scaled) < self.input_size:\n            pad_length = self.input_size - len(history_scaled)\n            history_scaled = np.pad(history_scaled, (pad_length, 0), 'constant')\n        \n        # Usamos los últimos input_size puntos para la predicción\n        x = history_scaled[-self.input_size:]\n        \n        # Convertir a tensor\n        x_tensor = torch.FloatTensor(x).unsqueeze(0).to(device)\n        \n        # Predicción recursiva multi-step\n        with torch.no_grad():\n            forecast = []\n            current_x = x_tensor\n            \n            # Necesitamos hacer predicciones en bloques de forecast_length\n            steps_done = 0\n            while steps_done < forecast_steps:\n                # Predecir el siguiente bloque\n                step_forecast = self.model(current_x).squeeze().cpu().numpy()\n                \n                # Añadir solo los pasos necesarios\n                remaining = forecast_steps - steps_done\n                steps_to_add = min(len(step_forecast), remaining)\n                forecast.extend(step_forecast[:steps_to_add])\n                steps_done += steps_to_add\n                \n                if steps_done >= forecast_steps:\n                    break\n                \n                # Actualizar x para la siguiente predicción\n                # Eliminamos los primeros valores y añadimos las nuevas predicciones\n                old_x = current_x.squeeze().cpu().numpy()\n                new_x = np.append(old_x[steps_to_add:], step_forecast[:steps_to_add])\n                current_x = torch.FloatTensor(new_x).unsqueeze(0).to(device)\n        \n        # Desnormalizar las predicciones\n        forecast = self.scaler.inverse_transform(np.array(forecast).reshape(-1, 1)).flatten()\n        \n        return forecast\n\ndef train_and_evaluate_nbeats(serie_mensual, municipio, input_window=12, forecast_window=6, test_size=6):\n    \"\"\"\n    Entrena y evalúa un modelo N-BEATS para un municipio específico\n    \"\"\"\n    # Filtrar datos del municipio\n    data = serie_semanal[serie_semanal['MUNICIPIO'] == municipio].copy()\n    \n    if len(data) < (input_window + forecast_window + test_size):\n        print(f\"¡Advertencia! No hay suficientes datos para el municipio {municipio}.\")\n        print(f\"Se necesitan al menos {input_window + forecast_window + test_size} puntos, pero solo hay {len(data)}.\")\n        return None, None, None, None\n    \n    # Ordenar por fecha\n    data = data.sort_values('ds')\n    \n    # Extraer la serie temporal (y)\n    y_values = data['y'].values\n    \n    # Asegurar que tenemos suficientes datos\n    if len(y_values) <= input_window + test_size:\n        print(f\"No hay suficientes datos para entrenar modelo para {municipio}\")\n        return None, None, None, None\n    \n    # Separar en train y test\n    train_data = y_values[:-test_size]\n    test_data = y_values[-test_size:]\n    dates_test = data['ds'].values[-test_size:]\n    \n    # Crear y entrenar el modelo\n    trainer = NBeatsTrainer(\n        input_size=input_window,\n        forecast_length=forecast_window,\n        hidden_layer_size=256,\n        nb_blocks_per_stack=4,\n        nb_layers=4,\n        nb_stacks=3,\n        batch_size=32\n    )\n    \n    print(f\"\\nEntrenando modelo N-BEATS para {municipio}...\")\n    train_losses = trainer.train(train_data, epochs=100, patience=10)\n    \n    # Generar predicciones\n    predictions = trainer.predict(train_data, forecast_steps=test_size)\n    \n    # Asegurar que las predicciones tienen la longitud correcta\n    if len(predictions) < len(test_data):\n        predictions = np.pad(predictions, (0, len(test_data) - len(predictions)), 'constant')\n    elif len(predictions) > len(test_data):\n        predictions = predictions[:len(test_data)]\n    \n    # Calcular métricas (con manejo de casos especiales)\n    mse = np.mean((predictions - test_data) ** 2)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(predictions - test_data))\n    \n    # MAPE con protección contra división por cero\n    non_zero_indices = test_data != 0\n    if np.any(non_zero_indices):\n        mape = np.mean(np.abs((test_data[non_zero_indices] - predictions[non_zero_indices]) / \n                               test_data[non_zero_indices])) * 100\n    else:\n        mape = float('inf')\n    \n    print(f\"\\nResultados para {municipio}:\")\n    print(f\"MSE: {mse:.4f}\")\n    print(f\"RMSE: {rmse:.4f}\")\n    print(f\"MAE: {mae:.4f}\")\n    print(f\"MAPE: {mape:.2f}%\")\n    \n    # Visualizar resultados\n    plt.figure(figsize=(12, 6))\n    \n    # Histórico (últimos input_window puntos)\n    hist_dates = data['ds'].values[-(input_window+test_size):-test_size]\n    hist_values = y_values[-(input_window+test_size):-test_size]\n    plt.plot(hist_dates, hist_values, 'b-', label='Histórico')\n    \n    # Valores reales en el período de prueba\n    plt.plot(dates_test, test_data, 'k-', label='Real')\n    \n    # Predicciones\n    plt.plot(dates_test, predictions, 'r--', label='Predicción')\n    \n    plt.title(f'Predicción de demanda para {municipio} (RMSE: {rmse:.2f})')\n    plt.xlabel('Fecha')\n    plt.ylabel('Cantidad')\n    plt.legend()\n    plt.grid(True)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n    \n    # Graficar pérdida de entrenamiento\n    if train_losses:\n        plt.figure(figsize=(10, 4))\n        plt.plot(train_losses)\n        plt.title('Pérdida durante entrenamiento')\n        plt.xlabel('Época')\n        plt.ylabel('Pérdida (MSE)')\n        plt.grid(True)\n        plt.show()\n    \n    return trainer, predictions, test_data, rmse\n\ndef multi_step_forecast(trainer, history, future_dates, periods=24):\n    \"\"\"\n    Genera una predicción multi-paso para varios períodos en el futuro\n    \n    Args:\n        trainer: Entrenador N-BEATS ya entrenado\n        history: Datos históricos completos\n        future_dates: Fechas futuras para la predicción\n        periods: Número total de períodos a predecir\n    \"\"\"\n    if trainer is None:\n        return None\n    \n    # Usar toda la historia disponible\n    forecast_values = trainer.predict(history, forecast_steps=periods)\n    \n    # Crear dataframe con las predicciones\n    forecast_df = pd.DataFrame({\n        'ds': future_dates[:len(forecast_values)],\n        'forecast': forecast_values\n    })\n    \n    return forecast_df\n\n# Ejemplo de uso:\n\n# Cargar datos ya preprocesados\nserie_mensual = pd.read_parquet('/kaggle/working/municipios_mensual_SinConc.parquet')\nmunicipios = serie_mensual['MUNICIPIO'].unique()\n\n# Seleccionar un municipio para entrenar\nmunicipio_elegido = municipios[81]\nprint(f\"Entrenando modelo para: {municipio_elegido}\")\n\n# Entrenar modelo para este municipio \ntrainer, preds, actuals, rmse = train_and_evaluate_nbeats(\n    serie_mensual, \n    municipio_elegido, \n    input_window=12,    # Usar 12 meses como entrada\n    forecast_window=6,  # Predecir 6 meses a la vez (este valor debe coincidir con lo que espera el modelo)\n    test_size=6         # Evaluar con los últimos 6 meses\n)\n\n\nif trainer is not None:\n    # Para un municipio específico, obtener todos sus datos\n    data_muni = serie_mensual[serie_mensual['MUNICIPIO'] == municipio_elegido]\n    history_values = data_muni['y'].values\n    \n    # Generar fechas futuras\n    last_date = data_muni['ds'].max()\n    future_dates = pd.date_range(start=pd.Timestamp(last_date) + pd.DateOffset(months=1), \n                                periods=12, freq='MS')\n    \n    # Predecir los próximos 24 meses\n    forecast_df = multi_step_forecast(trainer, history_values, future_dates, periods=12)\n    \n    if forecast_df is not None:\n        # Visualizar predicción futura\n        plt.figure(figsize=(14, 7))\n        plt.plot(data_muni['ds'], data_muni['y'], 'b-', label='Histórico')\n        plt.plot(forecast_df['ds'], forecast_df['forecast'], 'r--', label='Pronóstico')\n        plt.title(f'Pronóstico de demanda para {municipio_elegido} - Próximos 12 meses')\n        plt.xlabel('Fecha')\n        plt.ylabel('Cantidad')\n        plt.legend()\n        plt.grid(True)\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}